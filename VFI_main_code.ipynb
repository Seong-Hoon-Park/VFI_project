{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VFI_main_code.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"89060a67fa614630ad3632c168518d22":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_95daa82afaf644aa8c4df4e9cc7b5f8f","IPY_MODEL_2f5cfb218567403bba2efda9b0643d79","IPY_MODEL_419bee2e18044924a0497b2043ca2de2"],"layout":"IPY_MODEL_d5e9b322932946ab94eb018b06ba2675"}},"95daa82afaf644aa8c4df4e9cc7b5f8f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f41b9d4429a6431e8b1e82c9872b2825","placeholder":"​","style":"IPY_MODEL_dc8a18e340064ca592bf22da3674746e","value":"100%"}},"2f5cfb218567403bba2efda9b0643d79":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_46cb6b22f22240bba3d43642748c3cce","max":54276231,"min":0,"orientation":"horizontal","style":"IPY_MODEL_34adb05320994b7f8bf7dcd39b18e3aa","value":54276231}},"419bee2e18044924a0497b2043ca2de2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1824f4eed8fa4f60890c1422987b341c","placeholder":"​","style":"IPY_MODEL_14c3eb189810495385781ebf489e1225","value":" 51.8M/51.8M [00:06&lt;00:00, 11.0MB/s]"}},"d5e9b322932946ab94eb018b06ba2675":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f41b9d4429a6431e8b1e82c9872b2825":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc8a18e340064ca592bf22da3674746e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46cb6b22f22240bba3d43642748c3cce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34adb05320994b7f8bf7dcd39b18e3aa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1824f4eed8fa4f60890c1422987b341c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14c3eb189810495385781ebf489e1225":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18c9549e656e46b4a1c94b5bc9af53ca":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8f94e6a7df6546dfa752965a5957cc3e","IPY_MODEL_733c06d0de984e25af2e63f31dc1bdd0","IPY_MODEL_9fde42fb1c8e46f4a9be30c970d16cb1"],"layout":"IPY_MODEL_0659c89554a3431b95f96eb965f42812"}},"8f94e6a7df6546dfa752965a5957cc3e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_261d3966c97c46fd9a9873012535142a","placeholder":"​","style":"IPY_MODEL_1e0ca9f8f5f3427ab2df440a42aa68ab","value":"100%"}},"733c06d0de984e25af2e63f31dc1bdd0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a87ce7f4ef66444aab4992755e7307b4","max":553507836,"min":0,"orientation":"horizontal","style":"IPY_MODEL_939c02dd07b34a989eaf8006f9ccf567","value":553507836}},"9fde42fb1c8e46f4a9be30c970d16cb1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe6e2fb7ce894828a1bbe1f7160dd740","placeholder":"​","style":"IPY_MODEL_5b635603be8a42c09c34a539efafb71c","value":" 528M/528M [00:17&lt;00:00, 27.4MB/s]"}},"0659c89554a3431b95f96eb965f42812":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"261d3966c97c46fd9a9873012535142a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e0ca9f8f5f3427ab2df440a42aa68ab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a87ce7f4ef66444aab4992755e7307b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"939c02dd07b34a989eaf8006f9ccf567":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fe6e2fb7ce894828a1bbe1f7160dd740":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b635603be8a42c09c34a539efafb71c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Video Frame Interpolation Application on Game Video with fine-tuning."],"metadata":{"id":"rBxlXN049mVn"}},{"cell_type":"markdown","source":["본 코드의 3, 4, 5, 6, 7, 8, 9, 14에서는 이하의 참고 코드에서 대부분 혹은 완전한 코드를 인용하였으며, 10, 13, 15에서는 참고 논문의 내용을 이용하거나, 참고 코드의 일부를 인용하여 구현하였음을 밝힙니다.\n","\n","<br/>\n","\n","Reference\n","\n","Code: https://github.com/sniklaus/revisiting-sepconv/tree/942a018d853cfbe6510b716dd9cd10c13ec30650\n","\n","Paper: https://arxiv.org/pdf/2011.01280.pdf\n","\n","<br/>\n","\n","본 코드와 함께 'dataset' 폴더에서 train, validation, test set을 제공하고 있으나, etl의 업로드 용량 제한 문제로 극히 일부의 데이터만 포함하고 있습니다. 따라서, 코드의 실행을 확인할 수는 있지만, 성능을 확인하기는 어려운 점을 참고해주셨으면 합니다.\n","\n","실행환경은 colab이며 이외의 ipynb를 실행시킬 수 있는 환경이라면 작동 가능할 것으로 사료됩니다. 다만, requirements.txt에 있는 요구사항들이 설치되어 있어야 합니다.\n","\n","또한, 구글 드라이브 연동을 수행해야 하므로 FOLDERNAME을 반드시 현 코드가 위치한 폴더로 지정하여야 정상 작동합니다.\n","\n","<br/>\n","\n","본 코드를 통해서 train을 해보고 싶으시다면 코드의 1~14번을 순서대로 실행하여 train 후 train된 모델을 test set에 적용하고 그 정확도를 확인할 수 있습니다. 생성된 결과물은 './image_result/test_set_train_result/' 디렉토리에 'outxxxx.png' 형태로 저장됩니다. \n","\n","<br/>\n","\n","만약 best model을 테스트해보고 싶으시다면 코드의 1~12, 15를 순서대로 실행하여 test set에 대한 결과를 확인할 수 있습니다. 결과물은 './image_result/test_set_best_result/' 디렉토리에 'outxxxx.png' 형태로 저장됩니다.\n","\n","이때, 테스트는 './dataset/test' 디렉토리의 이미지들에 대하여 'train_t0(i).png'와 'train_t0(i+2).png'를 입력으로 하여 'train_t0(i+1).png'의 추정 이미지를 생성합니다. (이때, 괄호는 가독성을 위해 설명에서 추가한 것이고 실제 데이터명에 괄호는 붙지 않습니다). 즉 'out(i+1).png'는 'train_t0(i+1).png'에 대한 추정 결과로 생각할 수 있으며 similarity 역시 두 이미지를 비교한 값을 사용합니다."],"metadata":{"id":"_yCzKiDHI2lW"}},{"cell_type":"markdown","source":["### 1. Mount google drive."],"metadata":{"id":"sxdyn-__9zbv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1BxSDPb69DNa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655195030217,"user_tz":-540,"elapsed":23039,"user":{"displayName":"박성훈","userId":"11578462074453386740"}},"outputId":"4e4c20b0-7837-4c3b-a4de-df28f18c4e57"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/2022AI/VFI_Code\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)\n","# Have to target propriate folder\n","FOLDERNAME = '2022AI/VFI_Code'\n","\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","%cd /content/drive/MyDrive/$FOLDERNAME\n"]},{"cell_type":"markdown","source":["### 2. Download necessary packages."],"metadata":{"id":"n2_c-QHZ-UAy"}},{"cell_type":"code","source":["!pip install sewar"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K0wvZ35-gOI-","executionInfo":{"status":"ok","timestamp":1655195088542,"user_tz":-540,"elapsed":5334,"user":{"displayName":"박성훈","userId":"11578462074453386740"}},"outputId":"393b6a45-b55e-4e3e-bfb4-dcaee0b6e94c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sewar\n","  Downloading sewar-0.4.5.tar.gz (11 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sewar) (1.21.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sewar) (1.4.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from sewar) (7.1.2)\n","Building wheels for collected packages: sewar\n","  Building wheel for sewar (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sewar: filename=sewar-0.4.5-py3-none-any.whl size=10608 sha256=c6b5977f036d21c74c1775ba8bf38bf6aae83cb8848974bd7689fb9fe37d5af9\n","  Stored in directory: /root/.cache/pip/wheels/7a/38/6b/d066cfcb2b1c1c9b059b9ce1bdc4803b469359214e8dad4e3e\n","Successfully built sewar\n","Installing collected packages: sewar\n","Successfully installed sewar-0.4.5\n"]}]},{"cell_type":"markdown","source":["### 3. import necessary packages."],"metadata":{"id":"IkJxMJ2l-D1i"}},{"cell_type":"code","source":["import getopt\n","import math\n","import numpy\n","import PIL\n","import PIL.Image\n","import sys\n","import torch\n","import typing\n","import time\n","import copy\n","import random\n","import matplotlib.pyplot as plt\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from sewar.full_ref import uqi\n","\n","import sepconv # the custom separable convolution layer"],"metadata":{"id":"M-plnzWk96kd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4. Initial setting for torch and arguments."],"metadata":{"id":"rdsOHQNy_DM6"}},{"cell_type":"code","source":["##########################################################\n","\n","torch.set_grad_enabled(False) # make sure to not compute gradients for computational performance\n","\n","torch.backends.cudnn.enabled = True # make sure to use cudnn for computational performance\n","\n","##########################################################\n","\n","arguments_strModel = 'paper'\n","arguments_strOut = './image_result/test_set_train_result/out.png'\n","\n","##########################################################"],"metadata":{"id":"X0mFu8u3-_Mz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5. class Basic."],"metadata":{"id":"nZUO6DGw_OYC"}},{"cell_type":"code","source":["class Basic(torch.nn.Module):\n","    def __init__(self, strType:str, intChans:typing.List[int], objScratch:typing.Optional[typing.Dict]=None):\n","        super().__init__()\n","\n","        self.strType = strType\n","        self.netEvenize = None\n","        self.netMain = None\n","        self.netShortcut = None\n","\n","        intIn = intChans[0]\n","        intOut = intChans[-1]\n","        netMain = []\n","        intChans = intChans.copy()\n","        fltStride = 1.0\n","\n","        for intPart, strPart in enumerate(self.strType.split('+')[0].split('-')):\n","            if strPart.startswith('conv') == True:\n","                intKsize = 3\n","                intPad = 1\n","                strPad = 'zeros'\n","\n","                if '(' in strPart:\n","                    intKsize = int(strPart.split('(')[1].split(')')[0].split(',')[0])\n","                    intPad = int(math.floor(0.5 * (intKsize - 1)))\n","\n","                    if 'replpad' in strPart.split('(')[1].split(')')[0].split(','): strPad = 'replicate'\n","                    if 'reflpad' in strPart.split('(')[1].split(')')[0].split(','): strPad = 'reflect'\n","                # end\n","\n","                if 'nopad' in self.strType.split('+'):\n","                    intPad = 0\n","                # end\n","\n","                netMain += [torch.nn.Conv2d(in_channels=intChans[0], out_channels=intChans[1], kernel_size=intKsize, stride=1, padding=intPad, padding_mode=strPad, bias='nobias' not in self.strType.split('+'))]\n","                intChans = intChans[1:]\n","                fltStride *= 1.0\n","\n","            elif strPart.startswith('sconv') == True:\n","                intKsize = 3\n","                intPad = 1\n","                strPad = 'zeros'\n","\n","                if '(' in strPart:\n","                    intKsize = int(strPart.split('(')[1].split(')')[0].split(',')[0])\n","                    intPad = int(math.floor(0.5 * (intKsize - 1)))\n","\n","                    if 'replpad' in strPart.split('(')[1].split(')')[0].split(','): strPad = 'replicate'\n","                    if 'reflpad' in strPart.split('(')[1].split(')')[0].split(','): strPad = 'reflect'\n","                # end\n","\n","                if 'nopad' in self.strType.split('+'):\n","                    intPad = 0\n","                # end\n","\n","                netMain += [torch.nn.Conv2d(in_channels=intChans[0], out_channels=intChans[1], kernel_size=intKsize, stride=2, padding=intPad, padding_mode=strPad, bias='nobias' not in self.strType.split('+'))]\n","                intChans = intChans[1:]\n","                fltStride *= 2.0\n","\n","            elif strPart.startswith('up') == True:\n","                class Up(torch.nn.Module):\n","                    def __init__(self, strType):\n","                        super().__init__()\n","\n","                        self.strType = strType\n","                    # end\n","\n","                    def forward(self, tenIn:torch.Tensor) -> torch.Tensor:\n","                        if self.strType == 'nearest':\n","                            return torch.nn.functional.interpolate(input=tenIn, scale_factor=2.0, mode='nearest-exact', align_corners=False)\n","\n","                        elif self.strType == 'bilinear':\n","                            return torch.nn.functional.interpolate(input=tenIn, scale_factor=2.0, mode='bilinear', align_corners=False)\n","\n","                        elif self.strType == 'pyramid':\n","                            return pyramid(tenIn, None, 'up')\n","\n","                        elif self.strType == 'shuffle':\n","                            return torch.nn.functional.pixel_shuffle(tenIn, upscale_factor=2) # https://github.com/pytorch/pytorch/issues/62854\n","\n","                        # end\n","\n","                        assert(False) # to make torchscript happy\n","                    # end\n","                # end\n","\n","                strType = 'bilinear'\n","\n","                if '(' in strPart:\n","                    if 'nearest' in strPart.split('(')[1].split(')')[0].split(','): strType = 'nearest'\n","                    if 'pyramid' in strPart.split('(')[1].split(')')[0].split(','): strType = 'pyramid'\n","                    if 'shuffle' in strPart.split('(')[1].split(')')[0].split(','): strType = 'shuffle'\n","                # end\n","\n","                netMain += [Up(strType)]\n","                fltStride *= 0.5\n","\n","            elif strPart.startswith('prelu') == True:\n","                netMain += [torch.nn.PReLU(num_parameters=1, init=float(strPart.split('(')[1].split(')')[0].split(',')[0]))]\n","                fltStride *= 1.0\n","\n","            elif True:\n","                assert(False)\n","\n","            # end\n","        # end\n","\n","        self.netMain = torch.nn.Sequential(*netMain)\n","\n","        for strPart in self.strType.split('+')[1:]:\n","            if strPart.startswith('skip') == True:\n","                if intIn == intOut and fltStride == 1.0:\n","                    self.netShortcut = torch.nn.Identity()\n","\n","                elif intIn != intOut and fltStride == 1.0:\n","                    self.netShortcut = torch.nn.Conv2d(in_channels=intIn, out_channels=intOut, kernel_size=1, stride=1, padding=0, bias='nobias' not in self.strType.split('+'))\n","\n","                elif intIn == intOut and fltStride != 1.0:\n","                    class Down(torch.nn.Module):\n","                        def __init__(self, fltScale):\n","                            super().__init__()\n","\n","                            self.fltScale = fltScale\n","                        # end\n","\n","                        def forward(self, tenIn:torch.Tensor) -> torch.Tensor:\n","                            return torch.nn.functional.interpolate(input=tenIn, scale_factor=self.fltScale, mode='bilinear', align_corners=False)\n","                        # end\n","                    # end\n","\n","                    self.netShortcut = Down(1.0 / fltStride)\n","\n","                elif intIn != intOut and fltStride != 1.0:\n","                    class Down(torch.nn.Module):\n","                        def __init__(self, fltScale):\n","                            super().__init__()\n","\n","                            self.fltScale = fltScale\n","                        # end\n","\n","                        def forward(self, tenIn:torch.Tensor) -> torch.Tensor:\n","                            return torch.nn.functional.interpolate(input=tenIn, scale_factor=self.fltScale, mode='bilinear', align_corners=False)\n","                        # end\n","                    # end\n","\n","                    self.netShortcut = torch.nn.Sequential(Down(1.0 / fltStride), torch.nn.Conv2d(in_channels=intIn, out_channels=intOut, kernel_size=1, stride=1, padding=0, bias='nobias' not in self.strType.split('+')))\n","\n","                # end\n","\n","            elif strPart.startswith('...') == True:\n","                pass\n","\n","            # end\n","        # end\n","\n","        assert(len(intChans) == 1)\n","    # end\n","\n","    def forward(self, tenIn:torch.Tensor) -> torch.Tensor:\n","        if self.netEvenize is not None:\n","            tenIn = self.netEvenize(tenIn)\n","        # end\n","\n","        tenOut = self.netMain(tenIn)\n","\n","        if self.netShortcut is not None:\n","            tenOut = tenOut + self.netShortcut(tenIn)\n","        # end\n","\n","        return tenOut\n","    # end\n","# end"],"metadata":{"id":"80ESM48C_Oe3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6. class Encode."],"metadata":{"id":"Vl3jXCGT_UfO"}},{"cell_type":"code","source":["class Encode(torch.nn.Module):\n","    objScratch:typing.Dict[str, typing.List[int]] = None\n","\n","    def __init__(self, intIns:typing.List[int], intOuts:typing.List[int], strHor:str, strVer:str, objScratch:typing.Dict[str, typing.List[int]]):\n","        super().__init__()\n","\n","        assert(len(intIns) == len(intOuts))\n","        assert(len(intOuts) == len(intIns))\n","\n","        self.intRows = len(intIns) and len(intOuts)\n","        self.intIns = intIns.copy()\n","        self.intOuts = intOuts.copy()\n","        self.strHor = strHor\n","        self.strVer = strVer\n","        self.objScratch = objScratch\n","\n","        self.netHor = torch.nn.ModuleList()\n","        self.netVer = torch.nn.ModuleList()\n","\n","        for intRow in range(self.intRows):\n","            netHor = torch.nn.Identity()\n","            netVer = torch.nn.Identity()\n","\n","            if self.intOuts[intRow] != 0:\n","                if self.intIns[intRow] != 0:\n","                    netHor = Basic(self.strHor, [self.intIns[intRow], self.intOuts[intRow], self.intOuts[intRow]], objScratch)\n","                # end\n","\n","                if intRow != 0:\n","                    netVer = Basic(self.strVer, [self.intOuts[intRow - 1], self.intOuts[intRow], self.intOuts[intRow]], objScratch)\n","                # end\n","            # end\n","\n","            self.netHor.append(netHor)\n","            self.netVer.append(netVer)\n","        # end\n","    # end\n","\n","    def forward(self, tenIns:typing.List[torch.Tensor]) -> typing.List[torch.Tensor]:\n","        intRow = 0\n","        for netHor in self.netHor:\n","            if self.intOuts[intRow] != 0:\n","                if self.intIns[intRow] != 0:\n","                    tenIns[intRow] = netHor(tenIns[intRow])\n","                # end\n","            # end\n","            intRow += 1\n","        # end\n","\n","        intRow = 0\n","        for netVer in self.netVer:\n","            if self.intOuts[intRow] != 0:\n","                if intRow != 0:\n","                    tenIns[intRow] = tenIns[intRow] + netVer(tenIns[intRow - 1])\n","                # end\n","            # end\n","            intRow += 1\n","        # end\n","\n","        for intRow, tenIn in enumerate(tenIns):\n","            self.objScratch['levelshape' + str(intRow)] = tenIn.shape\n","        # end\n","\n","        return tenIns\n","    # end\n","# end"],"metadata":{"id":"bpgYkMDu_Uoc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###  7. class Decode"],"metadata":{"id":"7qh6xWtz_cOh"}},{"cell_type":"code","source":["class Decode(torch.nn.Module):\n","    objScratch:typing.Dict[str, typing.List[int]] = None\n","\n","    def __init__(self, intIns:typing.List[int], intOuts:typing.List[int], strHor:str, strVer:str, objScratch:typing.Dict[str, typing.List[int]]):\n","        super().__init__()\n","\n","        assert(len(intIns) == len(intOuts))\n","        assert(len(intOuts) == len(intIns))\n","\n","        self.intRows = len(intIns) and len(intOuts)\n","        self.intIns = intIns.copy()\n","        self.intOuts = intOuts.copy()\n","        self.strHor = strHor\n","        self.strVer = strVer\n","        self.objScratch = objScratch\n","\n","        self.netHor = torch.nn.ModuleList()\n","        self.netVer = torch.nn.ModuleList()\n","\n","        for intRow in range(self.intRows - 1, -1, -1):\n","            netHor = torch.nn.Identity()\n","            netVer = torch.nn.Identity()\n","\n","            if self.intOuts[intRow] != 0:\n","                if self.intIns[intRow] != 0:\n","                    netHor = Basic(self.strHor, [self.intIns[intRow], self.intOuts[intRow], self.intOuts[intRow]], objScratch)\n","                # end\n","\n","                if intRow != self.intRows - 1:\n","                    netVer = Basic(self.strVer, [self.intOuts[intRow + 1], self.intOuts[intRow], self.intOuts[intRow]], objScratch)\n","                # end\n","            # end\n","\n","            self.netHor.append(netHor)\n","            self.netVer.append(netVer)\n","        # end\n","    # end\n","\n","    def forward(self, tenIns:typing.List[torch.Tensor]) -> typing.List[torch.Tensor]:\n","        intRow = self.intRows - 1\n","        for netHor in self.netHor:\n","            if self.intOuts[intRow] != 0:\n","                if self.intIns[intRow] != 0:\n","                    tenIns[intRow] = netHor(tenIns[intRow])\n","                # end\n","            # end\n","            intRow -= 1\n","        # end\n","\n","        intRow = self.intRows - 1\n","        for netVer in self.netVer:\n","            if self.intOuts[intRow] != 0:\n","                if intRow != self.intRows - 1:\n","                    tenVer = netVer(tenIns[intRow + 1])\n","\n","                    if 'levelshape' + str(intRow) in self.objScratch:\n","                        if tenVer.shape[2] == self.objScratch['levelshape' + str(intRow)][2] + 1: tenVer = torch.nn.functional.pad(input=tenVer, pad=[0, 0, 0, -1], mode='constant', value=0.0)\n","                        if tenVer.shape[3] == self.objScratch['levelshape' + str(intRow)][3] + 1: tenVer = torch.nn.functional.pad(input=tenVer, pad=[0, -1, 0, 0], mode='constant', value=0.0)\n","                    # end\n","\n","                    tenIns[intRow] = tenIns[intRow] + tenVer\n","                # end\n","            # end\n","            intRow -= 1\n","        # end\n","\n","        return tenIns\n","    # end\n","# end"],"metadata":{"id":"tLty4Hhv_cUs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 8. class Network."],"metadata":{"id":"CBT6BG1q_g-X"}},{"cell_type":"code","source":["class Network(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.intEncdec = [1, 1]\n","        self.intChannels = [32, 64, 128, 256, 512]\n","\n","        self.objScratch = {}\n","\n","        self.netInput = torch.nn.Conv2d(in_channels=3, out_channels=int(round(0.5 * self.intChannels[0])), kernel_size=3, stride=1, padding=1, padding_mode='zeros')\n","\n","        self.netEncode = torch.nn.Sequential(*(\n","            [Encode([0] * len(self.intChannels), self.intChannels, 'prelu(0.25)-conv(3)-prelu(0.25)-conv(3)+skip', 'prelu(0.25)-sconv(3)-prelu(0.25)-conv(3)', self.objScratch)] +\n","            [Encode(self.intChannels, self.intChannels, 'prelu(0.25)-conv(3)-prelu(0.25)-conv(3)+skip', 'prelu(0.25)-sconv(3)-prelu(0.25)-conv(3)', self.objScratch) for intEncdec in range(1, self.intEncdec[0])]\n","        ))\n","\n","        self.netDecode = torch.nn.Sequential(*(\n","            [Decode([0] + self.intChannels[1:], [0] + self.intChannels[1:], 'prelu(0.25)-conv(3)-prelu(0.25)-conv(3)+skip', 'prelu(0.25)-up(bilinear)-conv(3)-prelu(0.25)-conv(3)', self.objScratch) for intEncdec in range(0, self.intEncdec[1])]\n","        ))\n","\n","        self.netVerone = Basic('up(bilinear)-conv(3)-prelu(0.25)-conv(3)', [self.intChannels[1], self.intChannels[1], 51])\n","        self.netVertwo = Basic('up(bilinear)-conv(3)-prelu(0.25)-conv(3)', [self.intChannels[1], self.intChannels[1], 51])\n","        self.netHorone = Basic('up(bilinear)-conv(3)-prelu(0.25)-conv(3)', [self.intChannels[1], self.intChannels[1], 51])\n","        self.netHortwo = Basic('up(bilinear)-conv(3)-prelu(0.25)-conv(3)', [self.intChannels[1], self.intChannels[1], 51])\n","\n","        self.load_state_dict(torch.hub.load_state_dict_from_url(url='http://content.sniklaus.com/resepconv/network-' + arguments_strModel + '.pytorch', file_name='resepconv-' + arguments_strModel))\n","    # end\n","\n","    def forward(self, tenSeq):\n","        assert(len(tenSeq) == 2)\n","\n","        tenOne = tenSeq[0]\n","        tenTwo = tenSeq[1]\n","\n","        with torch.set_grad_enabled(False):\n","            tenStack = torch.stack(tenSeq, 1)\n","            tenMean = tenStack.view(tenStack.shape[0], -1).mean(1, True).view(tenStack.shape[0], 1, 1, 1)\n","            tenStd = tenStack.view(tenStack.shape[0], -1).std(1, True).view(tenStack.shape[0], 1, 1, 1)\n","            tenSeq = [(tenFrame - tenMean) / (tenStd + 0.0000001) for tenFrame in tenSeq]\n","            tenSeq = [tenFrame.detach() for tenFrame in tenSeq]\n","        # end\n","\n","        tenOut = self.netDecode(self.netEncode([torch.cat([self.netInput(tenSeq[0]), self.netInput(tenSeq[1])], 1)] + ([0.0] * (len(self.intChannels) - 1))))[1]\n","\n","        tenOne = torch.nn.functional.pad(input=tenOne, pad=[int(math.floor(0.5 * 51)), int(math.floor(0.5 * 51)), int(math.floor(0.5 * 51)), int(math.floor(0.5 * 51))], mode='replicate')\n","        tenTwo = torch.nn.functional.pad(input=tenTwo, pad=[int(math.floor(0.5 * 51)), int(math.floor(0.5 * 51)), int(math.floor(0.5 * 51)), int(math.floor(0.5 * 51))], mode='replicate')\n","\n","        tenOne = torch.cat([tenOne, tenOne.new_ones([tenOne.shape[0], 1, tenOne.shape[2], tenOne.shape[3]])], 1).detach()\n","        tenTwo = torch.cat([tenTwo, tenTwo.new_ones([tenTwo.shape[0], 1, tenTwo.shape[2], tenTwo.shape[3]])], 1).detach()\n","\n","        tenVerone = self.netVerone(tenOut)\n","        tenVertwo = self.netVertwo(tenOut)\n","        tenHorone = self.netHorone(tenOut)\n","        tenHortwo = self.netHortwo(tenOut)\n","\n","        tenOut = sepconv.sepconv_func.apply(tenOne, tenVerone, tenHorone) + sepconv.sepconv_func.apply(tenTwo, tenVertwo, tenHortwo)\n","\n","        tenNormalize = tenOut[:, -1:, :, :]\n","        tenNormalize[tenNormalize.abs() < 0.01] = 1.0\n","        tenOut = tenOut[:, :-1, :, :] / tenNormalize\n","\n","        return tenOut\n","    # end\n","# end\n","\n","netNetwork = Network().cuda().eval()"],"metadata":{"id":"CHUlsNcS_hFL","colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["89060a67fa614630ad3632c168518d22","95daa82afaf644aa8c4df4e9cc7b5f8f","2f5cfb218567403bba2efda9b0643d79","419bee2e18044924a0497b2043ca2de2","d5e9b322932946ab94eb018b06ba2675","f41b9d4429a6431e8b1e82c9872b2825","dc8a18e340064ca592bf22da3674746e","46cb6b22f22240bba3d43642748c3cce","34adb05320994b7f8bf7dcd39b18e3aa","1824f4eed8fa4f60890c1422987b341c","14c3eb189810495385781ebf489e1225"]},"executionInfo":{"status":"ok","timestamp":1655195175236,"user_tz":-540,"elapsed":20262,"user":{"displayName":"박성훈","userId":"11578462074453386740"}},"outputId":"695ac876-e565-498f-e565-7af977f72896"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"http://content.sniklaus.com/resepconv/network-paper.pytorch\" to /root/.cache/torch/hub/checkpoints/resepconv-paper\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/51.8M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89060a67fa614630ad3632c168518d22"}},"metadata":{}}]},{"cell_type":"markdown","source":["### 9. estimate - get two frame as tensor and generate output frame with model."],"metadata":{"id":"KDP7PQql_q3P"}},{"cell_type":"code","source":["def estimate(tenOne, tenTwo):\n","\n","    assert(tenOne.shape[1] == tenTwo.shape[1])\n","    assert(tenOne.shape[2] == tenTwo.shape[2])\n","\n","    intWidth = tenOne.shape[2]\n","    intHeight = tenOne.shape[1]\n","\n","    assert(intWidth <= 1280) # while our approach works with larger images, we do not recommend it unless you are aware of the implications\n","    assert(intHeight <= 720) # while our approach works with larger images, we do not recommend it unless you are aware of the implications\n","\n","    tenPreprocessedOne = tenOne.cuda().view(1, 3, intHeight, intWidth)\n","    tenPreprocessedTwo = tenTwo.cuda().view(1, 3, intHeight, intWidth)\n","\n","    intPadr = (2 - (intWidth % 2)) % 2\n","    intPadb = (2 - (intHeight % 2)) % 2\n","\n","    tenPreprocessedOne = torch.nn.functional.pad(input=tenPreprocessedOne, pad=[0, intPadr, 0, intPadb], mode='replicate')\n","    tenPreprocessedTwo = torch.nn.functional.pad(input=tenPreprocessedTwo, pad=[0, intPadr, 0, intPadb], mode='replicate')\n","\n","    return netNetwork([tenPreprocessedOne, tenPreprocessedTwo])[0, :, :intHeight, :intWidth].cpu()\n","# end"],"metadata":{"id":"LdaYyceD_q-g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 10. Loss function"],"metadata":{"id":"Kf-VvRWSn_pd"}},{"cell_type":"code","source":["from torchvision.models import vgg16_bn\n","\n","# load VGG16 pre-trained model\n","vgg_m = vgg16_bn(True).features.cuda().eval()\n","vgg_m.requires_grad = False\n","\n","# get features with pre-trained VGG model\n","def get_features(x, model, f_layer):\n","    for name, layer in enumerate(model.children()): \n","        x = layer(x)\n","        if(name == f_layer):\n","            break\n","    return x\n","\n","# L2 loss with VGG perceptual loss\n","def image_loss_with_vgg(image_out, image_gt):    \n","    # preprocess image to get features     \n","    intWidth = image_gt.shape[2]\n","    intHeight = image_gt.shape[1]\n","    tenPreprocessedOut = image_out.cuda().view(1, 3, intHeight, intWidth)\n","    tenPreprocessedGt = image_gt.cuda().view(1, 3, intHeight, intWidth)\n","    intPadr = (2 - (intWidth % 2)) % 2\n","    intPadb = (2 - (intHeight % 2)) % 2\n","    tenPreprocessedGt = torch.nn.functional.pad(input=tenPreprocessedGt, pad=[0, intPadr, 0, intPadb], mode='replicate')\n","\n","    # get features with VGG pre-trained model\n","    out_features = get_features(tenPreprocessedOut, vgg_m, 5)\n","    gt_features = get_features(tenPreprocessedGt, vgg_m, 5)\n","\n","    feature_loss = torch.sum(torch.square(out_features - gt_features))\n","    return feature_loss\n","\n","# combination L1 loss with VGG perceptual loss (Used in our paper)\n","def image_loss_with_vgg2(image_out, image_gt, alpha=0.2, beta=0.01):    \n","    # preprocess image to get features     \n","    intWidth = image_gt.shape[2]\n","    intHeight = image_gt.shape[1]\n","    tenPreprocessedOut = image_out.cuda().view(1, 3, intHeight, intWidth)\n","    tenPreprocessedGt = image_gt.cuda().view(1, 3, intHeight, intWidth)\n","    intPadr = (2 - (intWidth % 2)) % 2\n","    intPadb = (2 - (intHeight % 2)) % 2\n","    tenPreprocessedGt = torch.nn.functional.pad(input=tenPreprocessedGt, pad=[0, intPadr, 0, intPadb], mode='replicate')\n","\n","    # get features with VGG pre-trained model\n","    out_features = get_features(tenPreprocessedOut, vgg_m, 5)\n","    gt_features = get_features(tenPreprocessedGt, vgg_m, 5)\n","\n","    feature_loss = beta*torch.sum(torch.abs(image_out - image_gt)) + alpha*torch.sum(torch.abs(out_features-gt_features))\n","    return feature_loss"],"metadata":{"id":"vQxNdimoYsXo","colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["18c9549e656e46b4a1c94b5bc9af53ca","8f94e6a7df6546dfa752965a5957cc3e","733c06d0de984e25af2e63f31dc1bdd0","9fde42fb1c8e46f4a9be30c970d16cb1","0659c89554a3431b95f96eb965f42812","261d3966c97c46fd9a9873012535142a","1e0ca9f8f5f3427ab2df440a42aa68ab","a87ce7f4ef66444aab4992755e7307b4","939c02dd07b34a989eaf8006f9ccf567","fe6e2fb7ce894828a1bbe1f7160dd740","5b635603be8a42c09c34a539efafb71c"]},"executionInfo":{"status":"ok","timestamp":1655195321791,"user_tz":-540,"elapsed":19660,"user":{"displayName":"박성훈","userId":"11578462074453386740"}},"outputId":"c4105bf0-ecab-4ca5-eb80-d7aee4f9d45c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to /root/.cache/torch/hub/checkpoints/vgg16_bn-6c64b313.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/528M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18c9549e656e46b4a1c94b5bc9af53ca"}},"metadata":{}}]},{"cell_type":"markdown","source":["### 11. UQI wrapper - UQI(Universal Quality Image Index, a kind of image similarity measurement)"],"metadata":{"id":"RgsQB22coYew"}},{"cell_type":"code","source":["def uqi_wrapper(out_image, image_gt):\n","    out_image_in_nparray = out_image.clip(0.0, 1.0).numpy().transpose(1, 2, 0)[:, :, ::-1]\n","    image_gt_in_nparray = image_gt.clip(0.0, 1.0).numpy().transpose(1, 2, 0)[:, :, ::-1]\n","    return uqi(out_image_in_nparray, image_gt_in_nparray)"],"metadata":{"id":"XnWVUWbLnpQp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 12. Load Dataset."],"metadata":{"id":"NmaOrlgaBSct"}},{"cell_type":"code","source":["dataset = None\n","# train:val = 5:1 in paper setting, reduced in here because of etl capacity limitation.\n","# type of each contructed data - [image1, image_gt, image2]\n","train_size = 22\n","valid_size = 20\n","train_start_index = 200\n","valid_start_index = [600, 720, 840, 960]\n","train_set_prefix = './dataset/train/train_t0'\n","valid_set_prefix = './dataset/val/train_t0'\n","\n","# loaded images\n","train_images = []\n","valid_images = []\n","\n","# contructed data set\n","train_set = []\n","valid_set = []\n","\n","for i in range(train_size):\n","    # load train image as tensor\n","    train_image = torch.FloatTensor(numpy.ascontiguousarray(numpy.array(PIL.Image.open(train_set_prefix + str(train_start_index+i+1).zfill(4) + '.png'))[:, :, ::-1].transpose(2, 0, 1).astype(numpy.float32) * (1.0 / 255.0)))\n","    train_images.append(train_image)\n","\n","for i in range(train_size-2):\n","    # construct train set\n","    train_set.append([train_images[i], train_images[i+1], train_images[i+2]])\n","\n","for k in range(4):\n","    valid_image = []\n","    for i in range(int(valid_size/4)):\n","        # load validation image as tensor\n","        valid_image = torch.FloatTensor(numpy.ascontiguousarray(numpy.array(PIL.Image.open(valid_set_prefix + str(valid_start_index[k]+i+1).zfill(4) + '.png'))[:, :, ::-1].transpose(2, 0, 1).astype(numpy.float32) * (1.0 / 255.0)))\n","        valid_images.append(valid_image)\n","\n","    for i in range(int(valid_size/4)-2):\n","        # construct validation set\n","        valid_set.append([valid_images[i],valid_images[i+1], valid_images[i+2]])\n","\n","# shuffle train set\n","random.shuffle(train_set)\n","dataset = {'train':train_set, 'val':valid_set}"],"metadata":{"id":"rUzg3okrBQwS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 13. Fine-Tuning\n","Fine-Tuning for game framges. \n","\n","Reference: https://tutorials.pytorch.kr/beginner/transfer_learning_tutorial.html"],"metadata":{"id":"EXFEawRUhKNq"}},{"cell_type":"code","source":["# image preprocessing\n","def preprocess_image(image1, image2):\n","    # image size equality check\n","    assert(image1.shape[1] == image2.shape[1])\n","    assert(image1.shape[2] == image2.shape[2])\n","\n","    intWidth = image1.shape[2]\n","    intHeight = image1.shape[1]\n","\n","    # image size limit check\n","    assert(intWidth <= 1280)\n","    assert(intHeight <= 720)\n","\n","    image1_preprocessed = image1.cuda().view(1, 3, intHeight, intWidth)\n","    image2_preprocessed = image2.cuda().view(1, 3, intHeight, intWidth)\n","\n","    intPadr = (2 - (intWidth % 2)) % 2\n","    intPadb = (2 - (intHeight % 2)) % 2\n","\n","    image1_preprocessed = torch.nn.functional.pad(input=image1_preprocessed, pad=[0, intPadr, 0, intPadb], mode='replicate')\n","    image2_preprocessed = torch.nn.functional.pad(input=image2_preprocessed, pad=[0, intPadr, 0, intPadb], mode='replicate')\n","    return intWidth, intHeight, image1_preprocessed, image2_preprocessed\n","\n","# train for game images\n","def train_vfi(model, optimizer, scheduler, num_epochs = 20, batch_size = 5):\n","    start = time.time()\n","    best_model = copy.deepcopy(model.state_dict())\n","    best_similarity = 0.0\n","    train_losses = []\n","    valid_losses = []\n","    train_size_in_batch = train_size-2\n","    valid_size_in_batch = valid_size-8\n","\n","    for epoch in range(num_epochs):\n","        print(f'Epoch {epoch+1}/{num_epochs}')\n","        if epoch % (int(train_size_in_batch / batch_size)) == 0:\n","            random.shuffle(dataset['train'])\n","\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                # model to train mode\n","                model.train()\n","            else:\n","                # model to evaluation mode\n","                model.eval()\n","            \n","            total_loss = 0.0\n","            total_similarity = 0.0\n","\n","            curr_dataset = None\n","            if phase == 'train':\n","                batch_start_index = (epoch*batch_size) % train_size_in_batch\n","\n","                curr_dataset = dataset[phase][batch_start_index:batch_start_index + batch_size]\n","            else:\n","                curr_dataset = dataset[phase]\n","\n","            for image1, image_gt, image2 in curr_dataset:\n","                image1 = image1.cuda()\n","                image2 = image2.cuda()\n","                image_gt = image_gt.cuda()\n","\n","                optimizer.zero_grad()\n","\n","                # foward\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    width, height, image1_p, image2_p = preprocess_image(image1, image2)\n","                    out_image = model([image1_p, image2_p])[0, :, :height, :width]\n","                    loss = image_loss_with_vgg2(out_image, image_gt)\n","                    sim = uqi_wrapper(torch.clone(out_image).cpu().detach(), torch.clone(image_gt).cpu().detach())\n","\n","                    # backpropagation in train mode\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","                    \n","                total_loss += loss\n","                total_similarity += sim\n","            \n","            if phase == 'train':\n","                scheduler.step()\n","\n","            dataset_size = 1\n","            if phase == 'train':\n","                dataset_size = batch_size\n","            else:\n","                dataset_size = valid_size_in_batch\n","\n","            # caculate epoch loss and similarity\n","            epoch_loss = total_loss / dataset_size\n","            epoch_sim = total_similarity / dataset_size\n","\n","            if phase == 'train':\n","                train_losses.append(epoch_loss.cpu())\n","            else:\n","                valid_losses.append(epoch_loss.cpu())\n","\n","            print(f'{phase} Loss: {epoch_loss:.4f}\\tSimilarity: {epoch_sim * 100:.2f}%')\n","\n","            # deep copy the better model\n","            if phase == 'val' and epoch_sim > best_similarity:\n","                best_similarity = epoch_sim\n","                best_model = copy.deepcopy(model.state_dict())\n","        print()\n","\n","\n","    time_elapsed = time.time() - start\n","    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n","    print(f'Best valid Acc: {best_similarity:4f}')\n","\n","    # plot train and valid loss\n","    xt = numpy.arange(1, num_epochs+1, 1)\n","    mint = min(int(min(train_losses)), int(min(valid_losses)))\n","    maxt = max(int(max(train_losses)), int(max(valid_losses)))\n","    plt.plot(xt, train_losses, '-bo', label='Train Loss')\n","    plt.plot(xt, valid_losses, '-ro', label='Valid Loss')\n","    plt.xlabel('epoch')\n","    plt.ylabel('Loss')\n","    plt.ylim(int(min(train_losses))-75, int(max(valid_losses))+75)\n","    plt.xticks(xt)\n","    plt.yticks(numpy.arange(int(min(train_losses))-50, int(max(valid_losses))+50, 50))\n","    plt.legend()\n","\n","    # load the best model weights\n","    model.load_state_dict(best_model)\n","    return model\n","\n","# train!\n","optimizer = optim.Adamax(netNetwork.parameters(), lr=0.0005, betas=(0.9, 0.999))\n","scheduler = lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.7)\n","best_model = train_vfi(netNetwork, optimizer, scheduler, 12)"],"metadata":{"id":"7HGgfjAqhP8i","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1654079989607,"user_tz":-540,"elapsed":2054671,"user":{"displayName":"박성훈","userId":"11578462074453386740"}},"outputId":"450e3938-b56c-4fea-99c4-7e6c893d6f4d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/12\n","train Loss: 955.3316\tSimilarity: 99.53%\n","val Loss: 972.4583\tSimilarity: 98.93%\n","\n","Epoch 2/12\n","train Loss: 912.9163\tSimilarity: 99.55%\n","val Loss: 971.9485\tSimilarity: 98.95%\n","\n","Epoch 3/12\n","train Loss: 952.6818\tSimilarity: 99.52%\n","val Loss: 988.5024\tSimilarity: 98.82%\n","\n","Epoch 4/12\n","train Loss: 961.5461\tSimilarity: 99.58%\n","val Loss: 976.2003\tSimilarity: 98.86%\n","\n","Epoch 5/12\n","train Loss: 929.0317\tSimilarity: 99.62%\n","val Loss: 970.1550\tSimilarity: 98.89%\n","\n","Epoch 6/12\n","train Loss: 945.3209\tSimilarity: 99.45%\n","val Loss: 977.9362\tSimilarity: 98.92%\n","\n","Epoch 7/12\n","train Loss: 901.4603\tSimilarity: 99.64%\n","val Loss: 964.3447\tSimilarity: 98.92%\n","\n","Epoch 8/12\n","train Loss: 914.5956\tSimilarity: 99.52%\n","val Loss: 962.1952\tSimilarity: 98.93%\n","\n","Epoch 9/12\n","train Loss: 895.1530\tSimilarity: 99.60%\n","val Loss: 962.5142\tSimilarity: 98.94%\n","\n","Epoch 10/12\n","train Loss: 919.4625\tSimilarity: 99.53%\n","val Loss: 960.1688\tSimilarity: 98.95%\n","\n","Epoch 11/12\n","train Loss: 905.3698\tSimilarity: 99.58%\n","val Loss: 956.9116\tSimilarity: 98.98%\n","\n","Epoch 12/12\n","train Loss: 929.1302\tSimilarity: 99.54%\n","val Loss: 958.0073\tSimilarity: 98.97%\n","\n","Training complete in 34m 14s\n","Best valid Acc: 0.989846\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["### 14. RUN!"],"metadata":{"id":"cJU6VZU4_zCZ"}},{"cell_type":"code","source":["arguments_strOne = './dataset/val/'\n","arguments_strTwo = './dataset/val/'\n","arguments_strMultih = './dataset/test/train_t0'\n","\n","multi_image = True\n","image_len = 50\n","image_start = 751\n","test_set_similarity = 0.0\n","\n","if arguments_strOut.split('.')[-1] in ['bmp', 'jpg', 'jpeg', 'png']:\n","    if multi_image:\n","        for i in range(image_start, image_start+image_len-2):\n","            tenOne = torch.FloatTensor(numpy.ascontiguousarray(numpy.array(PIL.Image.open(arguments_strMultih + str(i).zfill(4) + '.png'))[:, :, ::-1].transpose(2, 0, 1).astype(numpy.float32) * (1.0 / 255.0)))\n","            tenTwo = torch.FloatTensor(numpy.ascontiguousarray(numpy.array(PIL.Image.open(arguments_strMultih + str(i+2).zfill(4) + '.png'))[:, :, ::-1].transpose(2, 0, 1).astype(numpy.float32) * (1.0 / 255.0)))\n","\n","            tenOutput = estimate(tenOne, tenTwo)\n","            tenGt = torch.FloatTensor(numpy.ascontiguousarray(numpy.array(PIL.Image.open(arguments_strMultih + str(i+1).zfill(4) + '.png'))[:, :, ::-1].transpose(2, 0, 1).astype(numpy.float32) * (1.0 / 255.0)))\n","            test_set_similarity += uqi_wrapper(torch.clone(tenOutput).cpu().detach(), torch.clone(tenGt).cpu().detach())\n","\n","            PIL.Image.fromarray((tenOutput.clip(0.0, 1.0).numpy().transpose(1, 2, 0)[:, :, ::-1] * 255.0).astype(numpy.uint8)).save(arguments_strOut[:len(arguments_strOut)-4] + str(i+1).zfill(4) + '.png')\n","        \n","        print(f'test Acc: {test_set_similarity / image_len * 100:3f} %')\n","\n","\n","    else:\n","        ## generate just one frame from two images\n","        tenOne = torch.FloatTensor(numpy.ascontiguousarray(numpy.array(PIL.Image.open(arguments_strOne))[:, :, ::-1].transpose(2, 0, 1).astype(numpy.float32) * (1.0 / 255.0)))\n","        tenTwo = torch.FloatTensor(numpy.ascontiguousarray(numpy.array(PIL.Image.open(arguments_strTwo))[:, :, ::-1].transpose(2, 0, 1).astype(numpy.float32) * (1.0 / 255.0)))\n","\n","        tenOutput = estimate(tenOne, tenTwo)\n","        PIL.Image.fromarray((tenOutput.clip(0.0, 1.0).numpy().transpose(1, 2, 0)[:, :, ::-1] * 255.0).astype(numpy.uint8)).save(arguments_strOut)\n","# end"],"metadata":{"id":"vrzSc25Q_zJ7","executionInfo":{"status":"ok","timestamp":1654080699078,"user_tz":-540,"elapsed":392702,"user":{"displayName":"박성훈","userId":"11578462074453386740"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"85b8f27d-dd4f-4db9-fb1d-fc77f65805c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["test Acc: 98.907164 %\n"]}]},{"cell_type":"markdown","source":["### 15. Load best model and run for test set."],"metadata":{"id":"-4_NOyC89A47"}},{"cell_type":"code","source":["# Same with original Network class but load model from './best_weights/best_model_state_dict.pt'\n","class Network(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.intEncdec = [1, 1]\n","        self.intChannels = [32, 64, 128, 256, 512]\n","\n","        self.objScratch = {}\n","\n","        self.netInput = torch.nn.Conv2d(in_channels=3, out_channels=int(round(0.5 * self.intChannels[0])), kernel_size=3, stride=1, padding=1, padding_mode='zeros')\n","\n","        self.netEncode = torch.nn.Sequential(*(\n","            [Encode([0] * len(self.intChannels), self.intChannels, 'prelu(0.25)-conv(3)-prelu(0.25)-conv(3)+skip', 'prelu(0.25)-sconv(3)-prelu(0.25)-conv(3)', self.objScratch)] +\n","            [Encode(self.intChannels, self.intChannels, 'prelu(0.25)-conv(3)-prelu(0.25)-conv(3)+skip', 'prelu(0.25)-sconv(3)-prelu(0.25)-conv(3)', self.objScratch) for intEncdec in range(1, self.intEncdec[0])]\n","        ))\n","\n","        self.netDecode = torch.nn.Sequential(*(\n","            [Decode([0] + self.intChannels[1:], [0] + self.intChannels[1:], 'prelu(0.25)-conv(3)-prelu(0.25)-conv(3)+skip', 'prelu(0.25)-up(bilinear)-conv(3)-prelu(0.25)-conv(3)', self.objScratch) for intEncdec in range(0, self.intEncdec[1])]\n","        ))\n","\n","        self.netVerone = Basic('up(bilinear)-conv(3)-prelu(0.25)-conv(3)', [self.intChannels[1], self.intChannels[1], 51])\n","        self.netVertwo = Basic('up(bilinear)-conv(3)-prelu(0.25)-conv(3)', [self.intChannels[1], self.intChannels[1], 51])\n","        self.netHorone = Basic('up(bilinear)-conv(3)-prelu(0.25)-conv(3)', [self.intChannels[1], self.intChannels[1], 51])\n","        self.netHortwo = Basic('up(bilinear)-conv(3)-prelu(0.25)-conv(3)', [self.intChannels[1], self.intChannels[1], 51])\n","\n","        self.load_state_dict(torch.load('./best_weights/best_model_state_dict.pt'))\n","    # end\n","\n","    def forward(self, tenSeq):\n","        assert(len(tenSeq) == 2)\n","\n","        tenOne = tenSeq[0]\n","        tenTwo = tenSeq[1]\n","\n","        with torch.set_grad_enabled(False):\n","            tenStack = torch.stack(tenSeq, 1)\n","            tenMean = tenStack.view(tenStack.shape[0], -1).mean(1, True).view(tenStack.shape[0], 1, 1, 1)\n","            tenStd = tenStack.view(tenStack.shape[0], -1).std(1, True).view(tenStack.shape[0], 1, 1, 1)\n","            tenSeq = [(tenFrame - tenMean) / (tenStd + 0.0000001) for tenFrame in tenSeq]\n","            tenSeq = [tenFrame.detach() for tenFrame in tenSeq]\n","        # end\n","\n","        tenOut = self.netDecode(self.netEncode([torch.cat([self.netInput(tenSeq[0]), self.netInput(tenSeq[1])], 1)] + ([0.0] * (len(self.intChannels) - 1))))[1]\n","\n","        tenOne = torch.nn.functional.pad(input=tenOne, pad=[int(math.floor(0.5 * 51)), int(math.floor(0.5 * 51)), int(math.floor(0.5 * 51)), int(math.floor(0.5 * 51))], mode='replicate')\n","        tenTwo = torch.nn.functional.pad(input=tenTwo, pad=[int(math.floor(0.5 * 51)), int(math.floor(0.5 * 51)), int(math.floor(0.5 * 51)), int(math.floor(0.5 * 51))], mode='replicate')\n","\n","        tenOne = torch.cat([tenOne, tenOne.new_ones([tenOne.shape[0], 1, tenOne.shape[2], tenOne.shape[3]])], 1).detach()\n","        tenTwo = torch.cat([tenTwo, tenTwo.new_ones([tenTwo.shape[0], 1, tenTwo.shape[2], tenTwo.shape[3]])], 1).detach()\n","\n","        tenVerone = self.netVerone(tenOut)\n","        tenVertwo = self.netVertwo(tenOut)\n","        tenHorone = self.netHorone(tenOut)\n","        tenHortwo = self.netHortwo(tenOut)\n","\n","        tenOut = sepconv.sepconv_func.apply(tenOne, tenVerone, tenHorone) + sepconv.sepconv_func.apply(tenTwo, tenVertwo, tenHortwo)\n","\n","        tenNormalize = tenOut[:, -1:, :, :]\n","        tenNormalize[tenNormalize.abs() < 0.01] = 1.0\n","        tenOut = tenOut[:, :-1, :, :] / tenNormalize\n","\n","        return tenOut\n","    # end\n","# end\n","\n","best_model = Network().cuda().eval()"],"metadata":{"id":"UCgGMv8F9a9I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run for best model\n","arguments_strMultih = './dataset/test/train_t0'\n","arguments_strOut = './image_result/test_set_best_result/out.png'\n","\n","image_len = 50\n","image_start = 751\n","test_set_similarity = 0.0\n","\n","# estimate intermediate frame with best model\n","def estimate_on_best_model(tenOne, tenTwo):\n","\n","    assert(tenOne.shape[1] == tenTwo.shape[1])\n","    assert(tenOne.shape[2] == tenTwo.shape[2])\n","\n","    intWidth = tenOne.shape[2]\n","    intHeight = tenOne.shape[1]\n","\n","    assert(intWidth <= 1280) # while our approach works with larger images, we do not recommend it unless you are aware of the implications\n","    assert(intHeight <= 720) # while our approach works with larger images, we do not recommend it unless you are aware of the implications\n","\n","    tenPreprocessedOne = tenOne.cuda().view(1, 3, intHeight, intWidth)\n","    tenPreprocessedTwo = tenTwo.cuda().view(1, 3, intHeight, intWidth)\n","\n","    intPadr = (2 - (intWidth % 2)) % 2\n","    intPadb = (2 - (intHeight % 2)) % 2\n","\n","    tenPreprocessedOne = torch.nn.functional.pad(input=tenPreprocessedOne, pad=[0, intPadr, 0, intPadb], mode='replicate')\n","    tenPreprocessedTwo = torch.nn.functional.pad(input=tenPreprocessedTwo, pad=[0, intPadr, 0, intPadb], mode='replicate')\n","\n","    return best_model([tenPreprocessedOne, tenPreprocessedTwo])[0, :, :intHeight, :intWidth].cpu()\n","# end\n","\n","for i in range(image_start, image_start+image_len-2):\n","    # input frame is ith and (i+2)th frame\n","    tenOne = torch.FloatTensor(numpy.ascontiguousarray(numpy.array(PIL.Image.open(arguments_strMultih + str(i).zfill(4) + '.png'))[:, :, ::-1].transpose(2, 0, 1).astype(numpy.float32) * (1.0 / 255.0)))\n","    tenTwo = torch.FloatTensor(numpy.ascontiguousarray(numpy.array(PIL.Image.open(arguments_strMultih + str(i+2).zfill(4) + '.png'))[:, :, ::-1].transpose(2, 0, 1).astype(numpy.float32) * (1.0 / 255.0)))\n","\n","    tenOutput = estimate_on_best_model(tenOne, tenTwo)\n","    # result fram is (i+1)th frame\n","    tenGt = torch.FloatTensor(numpy.ascontiguousarray(numpy.array(PIL.Image.open(arguments_strMultih + str(i+1).zfill(4) + '.png'))[:, :, ::-1].transpose(2, 0, 1).astype(numpy.float32) * (1.0 / 255.0)))\n","    test_set_similarity += uqi_wrapper(torch.clone(tenOutput).cpu().detach(), torch.clone(tenGt).cpu().detach())\n","\n","    PIL.Image.fromarray((tenOutput.clip(0.0, 1.0).numpy().transpose(1, 2, 0)[:, :, ::-1] * 255.0).astype(numpy.uint8)).save(arguments_strOut[:len(arguments_strOut)-4] + str(i+1).zfill(4) + '.png')\n","\n","# print accuracy for test set\n","print(f'test Acc: {test_set_similarity / image_len * 100:3f} %')"],"metadata":{"id":"Re3PwTipaNDH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655196370957,"user_tz":-540,"elapsed":622725,"user":{"displayName":"박성훈","userId":"11578462074453386740"}},"outputId":"895af133-e89d-4422-b3ac-d161d2598dc0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["test Acc: 98.907164 %\n"]}]}]}
